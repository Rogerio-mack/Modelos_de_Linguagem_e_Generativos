{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKFAMfR5KahlQtDbadi+0m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rogerio-mack/Modelos_de_Linguagem_e_Generativos/blob/main/MLG_01_Introducao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Uma Introdução aos Grandes Modelos de Linguagem**"
      ],
      "metadata": {
        "id": "8Nn42nRe-lmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uma História Recente da IA\n",
        "\n",
        "* Antes: NLP, Processamento de Linguagem Natural\n",
        "* Agora: Modelos baseados em NN, Aprendizado Profundo\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://github.com/Rogerio-mack/Modelos_de_Linguagem_e_Generativos/blob/main/Hands-On-LLM/Figura1-1.png?raw=true?raw=true\" width=500>\n",
        "\n"
      ],
      "metadata": {
        "id": "CcZYCmZTHyy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK, Spacy\n",
        "\n",
        "> * *Tokens* (letra, sentença, palavra, etc.),\n",
        "> * stopwords,\n",
        "> * Lematização (redução à forma canônica),\n",
        "> * Stemming (redução à raiz da palavra),\n",
        "> * Reconhecimento de Entidades Nomeadas (NER),\n",
        "> * Identificação de Idioma,\n",
        "> * Parsing (análise sintática)\n",
        "> * *corpus*\n",
        "\n",
        "* **NLTK**: mais modular, maior controle sobre cada etapa do pipeline, mas  exige mais configuração.\n",
        "\n",
        "* **spaCy**: pipelines pré-treinados, para produção, menos flexibilidade para pesquisa.\n"
      ],
      "metadata": {
        "id": "4zOrLDiyaWcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK"
      ],
      "metadata": {
        "id": "JpA3Zf1naub2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import re\n",
        "\n",
        "# 1. Download de recursos\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 2. Texto de exemplo\n",
        "texto = \"A análise de sentimentos é uma tarefa importante em PLN.\"\n",
        "\n",
        "# 3. Inicializar recursos\n",
        "stop_words = set(stopwords.words('portuguese'))   # stopwords\n",
        "stemmer = RSLPStemmer()                           # stemmer\n",
        "\n",
        "# 4. Tokenização (nltk.word_tokenize)\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 5. Remover stopwords e pontuação\n",
        "tokens_limpos = [t for t in tokens if t.lower() not in stop_words and re.match(r'\\w+', t)]\n",
        "print(\"Tokens limpos:\", tokens_limpos)\n",
        "\n",
        "# 6. Stemming\n",
        "stems = [stemmer.stem(t) for t in tokens_limpos]\n",
        "print(\"Stems:\", stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP8VIgkOZPwk",
        "outputId": "c3021a0b-4af6-4676-a9c3-f089be923a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['A', 'análise', 'de', 'sentimentos', 'é', 'uma', 'tarefa', 'importante', 'em', 'PLN', '.']\n",
            "Tokens limpos: ['análise', 'sentimentos', 'tarefa', 'importante', 'PLN']\n",
            "Stems: ['anális', 'sent', 'taref', 'import', 'pln']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy"
      ],
      "metadata": {
        "id": "hz6OKbV0ayv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrsoF95VaAWs",
        "outputId": "d17a3a89-5ec3-47d1-e4bd-85dd7506ab51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# 1. Download e carregar modelo (pt_core_news_sm ou pt_core_news_md)\n",
        "#   python -m spacy download pt_core_news_sm    # (ou pt_core_news_md)\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# 2. Texto de exemplo\n",
        "texto = \"A análise de sentimentos é uma tarefa importante em PLN.\"\n",
        "\n",
        "# 3. Processar com spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# 4. Imprimir tokens, lemas e remover stopwords\n",
        "tokens_limpos = []\n",
        "for token in doc:\n",
        "    if not token.is_stop and not token.is_punct:\n",
        "        print(f\"Token: {token.text}, Lema: {token.lemma_}\")\n",
        "        tokens_limpos.append(token.lemma_)\n",
        "print(\"Lemas limpos:\", tokens_limpos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DqlOxzKZ21W",
        "outputId": "1b48b7cf-cb7f-43a7-ae0b-db39cd619c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: análise, Lema: análise\n",
            "Token: sentimentos, Lema: sentimento\n",
            "Token: tarefa, Lema: tarefa\n",
            "Token: importante, Lema: importante\n",
            "Token: PLN, Lema: PLN\n",
            "Lemas limpos: ['análise', 'sentimento', 'tarefa', 'importante', 'PLN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de Linguagem\n",
        "\n",
        "* **Modelos de Fundação**. Modelos de IA projetados para produzir para realizar uma série de tarefas incluindo geração de texto, vídeo, imagem ou áudio. Eles podem ser sistemas autônomos ou usados como uma \"base\" para outros aplicativos/modelos. Assim, por exemplo, o LLM chamado GPT funciona como o modelo de base do ChatGPT. O GPT é um modelo de Fundação.\n",
        "\n",
        "<img src=\"https://github.com/Rogerio-mack/Modelos_de_Linguagem_e_Generativos/blob/main/Hands-On-LLM/Figura1-2.png?raw=true?raw=true\" width=500>\n",
        "\n"
      ],
      "metadata": {
        "id": "Eu6yAeDzVu1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers, e algumas tarefas para Linguagem\n",
        "\n",
        "* **Tarefas de classificação (modelos apenas codificadores)**, classificam um texto ou parte dele.\n",
        "\n",
        "  * Análise de sentimento: Classifica o sentimento de um texto como positivo, negativo ou neutro.\n",
        "  * Classificação de texto: Atribui categorias ou rótulos pré-definidos a um texto.\n",
        "  * Reconhecimento de Entidades Nomeadas (NER): Identifica e categoriza entidades (como pessoas, organizações e locais) em um texto.\n",
        "  * Rotulagem de parte da fala (POS): Rotula cada palavra em uma frase com sua classe gramatical (substantivo, verbo, adjetivo, etc.).\n",
        "  * Classificação de sequência: Atribui uma etiqueta a uma sequência inteira de texto, útil para detectar spam ou a intenção de um usuário.\n",
        "  * Classificação de shot zero: Classifica um texto em uma categoria sem a necessidade de um modelo pré-treinado especificamente para essa categoria.\n",
        "\n",
        "* **Tarefas de geração (modelos apenas decodificadores)**, usados para gerar texto a partir de um prompt.\n",
        "\n",
        "  * Modelagem de linguagem (casual): Gera texto a partir de um início de frase, prevendo a próxima palavra com base nas anteriores.\n",
        "  * Geração de texto: Cria texto de forma livre, como poemas ou artigos.\n",
        "  * Preenchimento de máscara: Preenche as palavras ausentes em uma frase (modelagem de linguagem mascarada).\n",
        "\n",
        "* **Tarefas de sequência a sequência (modelos codificador-decodificador)**, modelos recebem uma entrada de texto e produzem uma saída de texto.\n",
        "\n",
        "  * Tradução automática: Traduz texto de um idioma para outro.\n",
        "  * Sumarização de texto: Gera um resumo conciso de um texto mais longo.\n",
        "  * Resposta a perguntas: Extrai uma resposta de um texto com base em uma pergunta (extrativa) ou gera uma resposta a partir do conhecimento do modelo (generativa).\n",
        "\n",
        "> **Acesse:** https://huggingface.co/models > Tasks"
      ],
      "metadata": {
        "id": "QIbjyzq5eMpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Especifica um modelo para análise de sentimento em português\n",
        "classifier_pt = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "\n",
        "# Exemplo de uso com texto em português\n",
        "result = classifier_pt(\"Eu tô amando este curso de Modelos de Linguagem!\")\n",
        "print(result)\n",
        "\n",
        "result = classifier_pt(\"Mas eu odeio quando preciso programar!\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "4261c5a64b7b4089bf55d7dd79d7e747",
            "508dd397e868471a85b0aad9f71d83b1",
            "055b6218b88941b59b2f93dc29e39d35",
            "5a17f5c3924e4ffeb00d8ae40e3b3e33",
            "f12b999a6be84749981c21a1adc3ba9a",
            "537111eacf0444708c2a648296c201eb",
            "1e5181c6a63945a2832f5ae795b1a6fa",
            "5225a359979a4241973865abb0855191",
            "9ffb44e374bf4d3f86136a2eaa477cb4",
            "43641ee107274437a47d508b37828c80",
            "9a7dcfd8217c4b90bd019fad0972cd38",
            "f4ccabe84cf14e9e9f1096468d392b50",
            "9909c357b5fd4b2db349e5ba9c6e98a4",
            "ea1e2235faa64f3eaefb7848c933c4d6",
            "5b84aa6c9d2142c5a82c4e02e5166173",
            "06b3cad2cd0842d999ebd77ef7aba643",
            "a473dc757db143068c121ab16ffc0504",
            "f3ea44ac54084438903fbde4e3e946ed",
            "5973ddf19a1e406abeb1c7edd58a5d78",
            "abd1822d9678444cbee389797a62b4d2",
            "4217586322694f049a733531e4ec90ec",
            "6f2f474d82ec414bb9c7f2515c9098a9",
            "59db3f1c074e4245ad6e68c8149e5303",
            "081c625a90824b2b952fa72fdd9b9572",
            "d50f4b8067404fbb97c5628eabdd8525",
            "c409a1b2c85f49e5af6095a44f98a3b8",
            "cea64424b48942c49d5d3b07361a013e",
            "ef48fd2605cd4901b958a4b35deb38af",
            "d93c6b73a5e84a9b9ef4fbc6f12b894a",
            "dcd7fdb43ce044bdb7f27af947b5e92f",
            "60bda26b93ea4208aaadbf7db659108e",
            "fb77134604d64cc0bfb93a47a92336e1",
            "dfff20452fae47fea8d5b407c76ebca1",
            "24b02a96dd6b4945b89a4ad0b7c26884",
            "d0d6ad912d384fd48a672b333c9d115b",
            "99399aa34c774fac8cc0d8edfdb4306b",
            "0bdbe32440664f42b107a4eb026281c0",
            "1194acdab47c4c40a999f2973d824140",
            "44015360597e48d8981a44a0b99f3614",
            "dda0ed26651146df8326b6f348f3e828",
            "8334b5fa913f4c41b737b68108b4ecbc",
            "13d8d38106d34683b5cbfce54ccffbc2",
            "b7ee8fd903f846938a43d17fb041d598",
            "830b755a6e9348f48db76ba6b70fbdba",
            "2e3d8209b175456ab24f90bfafeb9319",
            "d914b6a329494f1dbcdd406f8f8e5948",
            "7ab3ef228dde4e71a6aaa2392204c5e7",
            "78b7cc9dc5fa4eb189ce07a722c81475",
            "d3ac49e7878d4c039c51acfda7f9be70",
            "468ce398a56241e9ba7766324a7725e6",
            "bb05af824a0745cb94719773f15d3e0b",
            "bde2eccf165c4842877521e50ff54e3d",
            "fdead0360471406689b2149f37a8b334",
            "4ab5caa590fc47088b69ef9af8a9ba4c",
            "ef10bdf919944edba3ee7f798477dab9"
          ]
        },
        "id": "vWCOwm1hez4g",
        "outputId": "4df5ff75-74ff-4d36-e64c-2ca29c4d0bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4261c5a64b7b4089bf55d7dd79d7e747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4ccabe84cf14e9e9f1096468d392b50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59db3f1c074e4245ad6e68c8149e5303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24b02a96dd6b4945b89a4ad0b7c26884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e3d8209b175456ab24f90bfafeb9319"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'positive', 'score': 0.9205960035324097}]\n",
            "[{'label': 'negative', 'score': 0.9149842858314514}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Fill-Mask (Preenchimento de Máscara)\n",
        "fill_masker = pipeline(\"fill-mask\", model=\"neuralmind/bert-base-portuguese-cased\")\n",
        "# Use [MASK] como token de máscara para este modelo\n",
        "masked_text_pt = \"O Brasil não é um país [MASK].\"\n",
        "predictions = fill_masker(masked_text_pt, top_k=3)\n",
        "print(\"\\nFill Mask:\")\n",
        "for p in predictions:\n",
        "    print(f\"Token: {p['token_str']}, Score: {p['score']:.4f}\")\n",
        "\n",
        "masked_text_pt = \"Mais vale um [MASK] na mão que dois voando.\"\n",
        "predictions = fill_masker(masked_text_pt, top_k=3)\n",
        "print(\"\\nFill Mask:\")\n",
        "for p in predictions:\n",
        "    print(f\"Token: {p['token_str']}, Score: {p['score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krcO7CE8gSaG",
        "outputId": "9c5d5880-89c9-49cd-d365-33c792212d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fill Mask:\n",
            "Token: sério, Score: 0.1059\n",
            "Token: democrático, Score: 0.0689\n",
            "Token: isolado, Score: 0.0449\n",
            "\n",
            "Fill Mask:\n",
            "Token: avião, Score: 0.2268\n",
            "Token: pássaro, Score: 0.0564\n",
            "Token: coração, Score: 0.0408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Por que tratar linguagem natural é importante?\n",
        "\n",
        "* Produção de informação não estruturada, escrita e falada\n",
        "\n",
        "* Tarefas automatizadas de classificação, clusterização etc.\n",
        "\n",
        "* Aplicações: search engines, sentiment analysis, classificação automática de notícias, emails, artigos, reclamações, bots, reconhecimento de voz, tradução automática etc.\n"
      ],
      "metadata": {
        "id": "VTGETx58il3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Por que uma representação por vetores?\n",
        "\n",
        "* Precisamos de uma representação estruturada para manipular e processar as informações\n",
        "\n",
        "* **Luhn** (1953?) Argumentou que as palavras muito frequentes e as pouco frequentes não colaboram para discriminação e similaridade entre documentos.\n",
        "\n",
        "* Frequencia dos termos = sentido dos documentos\n",
        "\n",
        "* Fácil manipulação = funções de similaridade"
      ],
      "metadata": {
        "id": "mGYpMJLJizIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principais formas de representação da Linguagem\n",
        "\n",
        "* **BOW**, Bag of Words  \n",
        "\n",
        "* **TF-IDF**, term frequency–inverse document frequency\n",
        "\n",
        "* **Word Embedding (2013, Tomas Mikolov at Google)**\n",
        "\n",
        "* **Transformers**, Bert (2018, Bidirectional Encoder Representations from Transformers, Google) e concorrentes..."
      ],
      "metadata": {
        "id": "kbsHjqiijrNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BOW**, Bag of Words\n",
        "\n",
        "<img src=\"https://github.com/Rogerio-mack/Modelos_de_Linguagem_e_Generativos/blob/main/figures/bow.png?raw=true?raw=true\" width=600>\n",
        "\n",
        "<br>\n",
        "\n",
        "Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284.\n"
      ],
      "metadata": {
        "id": "lZUwum3Smp0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distância (Similaridade) Cosseno\n",
        "\n",
        "A similaridade cosseno entre dois vetores, A e B, é dada por:\n",
        "\n",
        "$$\\text{similaridade_cosseno}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
        "\n",
        "Em que:\n",
        "\n",
        "- $A \\cdot B$ é o produto escalar (dot product) de A e B.\n",
        "- $||A||$ e $||B||$ são as normas (magnitude) de A e B.\n",
        "\n",
        "<br>\n",
        "\n",
        "O valores da similaridade variam de -1 a 1:\n",
        "\n",
        "- 1: Vetores apontam na mesma direção (máxima similaridade).\n",
        "- 0: Vetores são ortogonais (não há similaridade).\n",
        "- -1: Vetores apontam em direções opostas (máxima dissimilaridade)."
      ],
      "metadata": {
        "id": "5N_13zyHluTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF-IDF**, term frequency–inverse document frequency\n",
        "\n",
        "<img src=\"https://github.com/Rogerio-mack/Modelos_de_Linguagem_e_Generativos/blob/main/figures/tfidf.png?raw=true?raw=true\" width=1000>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1KFj4axnkkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequência do termo (TF)\n",
        "\n",
        "\n",
        "* Contagem bruta\n",
        "\n",
        "$$ \\mathrm{TF}(t,d) = f(t,d) $$\n",
        "\n",
        "* Frequência relativa (normalizada pelo tamanho do documento)\n",
        "\n",
        "$$ \\mathrm{TF}(t,d) = \\frac{f(t,d)}{\\sum_{t' \\in d} f(t',d)} $$\n",
        "\n",
        "Em que $f(t,d)$ é o número de vezes que o termo $t$ aparece no documento $d$, e o denominador o número total de termos no documentos.\n",
        "\n"
      ],
      "metadata": {
        "id": "xqpOWCiA9ET4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequência inversa do documento (IDF)\n",
        "\n",
        "$$\\mathrm{IDF}(t,D) = \\log \\frac{N}{n_t} $$\n",
        "\n",
        "Em que $N$ é o número total de documentos e $n_t$ é o número de documentos contendo o termo $t$ na coleção $D$. Havendo ainda a variante suavizada $\\log \\frac{N+1}{n_t+1}$ para evitar divisões por zero."
      ],
      "metadata": {
        "id": "s1EluiaH9qgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Frequência do termo – frequência inversa do documento**\n",
        "\n",
        "$$ \\mathrm{TF\\_IDF}(t,d,D) = \\mathrm{TF}(t,d) \\space \\mathrm{IDF}(t,D) = \\frac{f(t,d)}{\\sum_{t' \\in d} f(t',d)} \\log \\frac{N}{n_t}  $$\n"
      ],
      "metadata": {
        "id": "jttgU2S6_FMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O termo $IDF$, também chamado de *especificidade do termo* fornece um peso inverso à quantidade de vezes que o termo aparece no documento e portanto, reduz o \"peso\" de termos muito frequentes na coleção."
      ],
      "metadata": {
        "id": "zf3jndjs_83Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O que podemos fazer agora com isso?\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/Rogerio-mack/Modelos_de_Linguagem_e_Generativos/blob/main/figures/tfidf_ml.png?raw=true?raw=true\" width=1000>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "* Classificação\n",
        "* Clusterização\n",
        "* ...e quaisquer outras tarefas de aprendizado de máquina."
      ],
      "metadata": {
        "id": "gpaYFw7aoJky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Word Embedding\n",
        "\n",
        "Uma representação melhor pode ser obtida com a técnica de Word Embedding, e veremos essa técnica mais adiante.\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/posts/2017-10-15-word-embedding/word2vec-skip-gram.png\" width=500>\n",
        ""
      ],
      "metadata": {
        "id": "fuuTFdvlrGmW"
      }
    }
  ]
}